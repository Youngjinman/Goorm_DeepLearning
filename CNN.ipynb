{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7dbb87",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "    이미지 인식을 위해 뽑아내야 하는 low level 의 정보들을 뽑아내는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8eb49",
   "metadata": {},
   "source": [
    "# CNN operation\n",
    "    target 이미지와 동일한 패턴을 찾아내는 것이 아니라 target 이미지가 지니는 부분적인 패턴들을 정의하고 그 패턴들을 지니고 있는지를 확인함.\n",
    "\t그 패턴을 convolutional filter이라고 하고 입력 데이터를 필터가 순회하며 픽셀끼리의 곱을 합산하여(내적) 필터의 픽셀의 수로 나눈다.\n",
    "\t필터가 해당이미지를 모두 순회하면 내적의 평균값들이 필터가 순회한 만큼 나온다. 그것을 mapping 하여 새로운 map을 생성한 것이 activation map이라고 함\n",
    "\t.77 -> 77% 매칭. 1 -> 100% 매칭\n",
    "\tactivation map의 값 하나하나는 선형결합으로 생겨난 것. 예) activation map의 첫번째 값은 convolutional filter와 이미지 맨왼쪽맨위의 9개의(예를 들면) 픽셀과의 내적의 합의 평균의 결과이다.\n",
    "\t따라서 이것을 neural net으로 생각해보면 입력노드는 이미지의 9개의 픽셀이고 가중치는 convolutional filter이고 출력노드는 activation map의 값 하나이다.\n",
    "\t-> convolution layer\n",
    "\tconvolutional filter는 3*3이지만 RGB이므로 3개가 쌓여있다. 쌓여있는 filter을 activation map에 순회를 하면 단순히 처음의 출력값보다 주변 픽셀들의 값을 더 고려한 high level 의 정보를 담게 된다.\n",
    "\thidden layer가 쌓일수록 주변 픽셀들의 정보를 고려하게 되므로 점점 이미지를 잘 맞출 수 있게 되는 것이다.\n",
    "\tconv-relu-pooling 의 과정이 일반적인 CNN의 운영과정이다. 합성곱 진행후 RELU 활성화함수 통과 Pooling 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5cd28",
   "metadata": {},
   "source": [
    "# Max Pooling\n",
    "    activation map을 특정 window크기로 순회하며 가장 큰 값만을 뽑아냄. 가장 우세한 값을 뽑아냄으로써 target이미지와 완벽히 일치하지 않아도 \n",
    "\t해당 패턴을 지니는 위치가 비슷하고 그 정도의 세기로 나타냈음을 알려주게된다. 전체 이미지 사이즈도 줄게 됨. \n",
    "\tactivation map이 max pooling을 통해 출력을 내는 층을 pooling layer 라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ab5dd",
   "metadata": {},
   "source": [
    "# Pre-trained Model\n",
    "    이미 모델구조가 fix되어있고 최적화된 파라미터가 있는 상태에서 초반 레이어는 edge정도를 생성하기 때문에 대부분 task에서 재사용될 수 있음.\n",
    "\t추후에 본인의 task에 맞게끔 pre-trained model에 hidden layer을 추가하고 그 부분의 파라미터만 학습시켜 task를 수행하는 것을 transfer learning(전이 학습)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d656d",
   "metadata": {},
   "source": [
    "# VGG-network\n",
    "    100*100*64(depth)의 input image를 100*100*64의 output 으로 하려면 filter의 파라미터의 수는 가로세로를 7*7로 한다면 7*7*64 여기까지가 input을 순회하는 한개의 필터고, 필터가 64개가 필요하므로 총 파라미터는 7*7*64*64가 된다.\n",
    "\t3*3*depth의 filter을 여러번 거치면 input image에 대해서는 7*7*depth 정도의 receptive field를 커버할 수 있다(범위가 계속 누적되므로). \n",
    "\t처음부터 7*7*depth filter을 쓰는 것 보다 3*3*depth filter을 쓰고 layer을 늘리는 것이 파라미터 수도 적고 (3*3*64 여기까지가 한개의 필터 3*3*64*64 (64개의 필터가 필요하므로) 여기에 layer가 3이라면 3*3*64*64*3이 된다.) \n",
    "\t중간에 non linearity를 활용할 수 있으므로 VGG-net이 낫다.\n",
    "\t위의 파라미터 연산이 중요함. input 이미지를 순회하는 필터의 가로세로를 n,m이라고 하고 input 이미지의 depth를 C라고 하고 필터의 개수를 C'라고 하면\n",
    "\t필요한 파라미터는 n*m*C*C'이 된다. conv후의 output인 activation map의 크기는 가로세로 설정에 따라 다른데, 가로*세로*필터의 개수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0610ef5",
   "metadata": {},
   "source": [
    "# Google-net\n",
    "    1*1 convolution: demension을 줄여주는 pre process로 많이 쓰임(파라미터 수 감소 가능)\n",
    "\tpooling 과정에서도 가로세로를 줄이지 않고 stride와 padding을 조절하여 가로세로 유지 가능\n",
    "\t가로세로가 동일한 여러 activation map을 다시 concatenate하여 depth가 깊은 output 생성\n",
    "\n",
    "\tMulti-task Learning: 학습을 진행하다가 두갈래로 output을 나눔\n",
    "\t\t-> 두갈래로 나누기 전까지는 일반적인 feature을 뽑고 갈래로 들어갈수록 specific한 특성을 뽑아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a418e33",
   "metadata": {},
   "source": [
    "# Res-Net\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
